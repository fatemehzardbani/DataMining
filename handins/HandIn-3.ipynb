{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand In 3 - Frequent Itemsets, Random Walks and Sequence Segmentation\n",
    "Due: May 15th 2020, 23:59\n",
    "\n",
    "This is a mandatory handin to be done in groups of 2-3 students, who shall submit:\n",
    "1. A report in **PDF format**. The report should contain your experimental results; and\n",
    "2. Python code in a zip-file. \n",
    "\n",
    "The pdf should **not** be in the zip-file and your report should **not** be a part of the python code. \n",
    "In other words, your report should be self-contained and your code should be there to document\n",
    "that you actually did what you claim :-).\n",
    "\n",
    "Submission should be done in Blackboard by **May 15th 23.59**.\n",
    "\n",
    "## Problem 1 - Frequent Itemsets\n",
    "We have learned the Apriori and FP-Growth algorithms for mining frequent itemsets.\n",
    "\n",
    "1. Develop an implementation of both.\n",
    "2. Run an experiment and show to what extent FP-Growth has an advantage.\n",
    "\n",
    "Obtain the anonymized real-world `retail market basket` data from: http://fimi.ua.ac.be/data/.\n",
    "This data comes from an anonymous Belgian retail store, and was donated by Tom Brijs from Limburgs Universitair Centrum, Belgium. The original data contains 16,470 different items and 88,162 transactions. You may only work with the top-50 items in terms of occurrence frequency.\n",
    "\n",
    "_Hint:_ We have used this dataset before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# Local imports\n",
    "import sys\n",
    "sys.path.append('../utilities')\n",
    "from load_data import load_market_basket, load_dblp_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the retail data\n",
    "transactions = load_market_basket()\n",
    "\n",
    "def filter_transactions(T, k=50):\n",
    "    \"\"\"\n",
    "        Keep only the top k items in the transactions.\n",
    "        Remove transactions that become empty.\n",
    "    \"\"\"\n",
    "    # Count occurences of each item\n",
    "    counts = [0] * 16470\n",
    "    for t in T:\n",
    "        for i in t:\n",
    "            counts[i] += 1\n",
    "\n",
    "    # Sort and select top k\n",
    "    counts = np.array(counts)\n",
    "    order  = np.argsort(counts)[::-1] # reverse the sorted order\n",
    "\n",
    "    indexes_to_keep = order[:k]       # Keep the top k items\n",
    "    index_set = set(indexes_to_keep)  # Convert to python set for efficiency\n",
    "\n",
    "    # Filter transactions\n",
    "    T_new = [t_ for t_ in  [list(filter(lambda i: i in index_set, t)) for t in T]  if t_]\n",
    "    return T_new\n",
    "\n",
    "T = filter_transactions(transactions, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny function for generating rules from tuples\n",
    "# Ex: rule((1, 2), (5)) outputs \"(1, 2) => (5)\"\n",
    "rule  = lambda lhs, rhs: \"%s => %s\" % (str(lhs), str(rhs)) # For generating rule strings\n",
    "\n",
    "\n",
    "def listify(L):\n",
    "    return [[l] for l in L]\n",
    "\n",
    "def get_occurences(I, T):\n",
    "    occurences = 0\n",
    "    for t in T:\n",
    "        contains_all = True\n",
    "        for i in I:\n",
    "            if not i in t:\n",
    "                contains_all = False\n",
    "                break\n",
    "        if contains_all:\n",
    "            occurences += 1\n",
    "            \n",
    "    return occurences\n",
    "            \n",
    "def get_all_items(T):\n",
    "    items = []\n",
    "    for t in T:\n",
    "        for i in t:\n",
    "            if not i in items:\n",
    "                items.append(i)\n",
    "    return items\n",
    "\n",
    "def is_frequent(I, T, support):\n",
    "    return get_occurences(I, T) / len(T) >= support\n",
    "\n",
    "\n",
    "def join_pair(L1, L2):\n",
    "    result = []\n",
    "    \n",
    "    for l1 in L1:\n",
    "        result.append(l1)\n",
    "    for l2 in L2:\n",
    "        if not l2 in result:\n",
    "            result.append(l2)\n",
    "            \n",
    "    return result\n",
    "\n",
    "def contains(candidate, C_k):\n",
    "    return get_occurences(candidate, C_k) > 0\n",
    "\n",
    "def flatten(L):\n",
    "    \n",
    "    flat_list = []\n",
    "    for sublist in L:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "            \n",
    "    return flat_list\n",
    "   \n",
    "\n",
    "def apriori_algorithm(T, support=0.05, min_confidence=0.7):\n",
    "    \"\"\"\n",
    "        Apriori algorithm for mining association rules.\n",
    "        Inputs:\n",
    "            T:               A list of lists, each inner list will contiain integer-item-ids. \n",
    "                             Example: T = [[1, 2, 5], [2, 3, 4], [1, 6]]\n",
    "            support:         The proportion of occurences needed to keep itemsets.\n",
    "            min_confidence:  Minimum confidence for the algorithm to output the rule.\n",
    "        \n",
    "        Outputs:\n",
    "            rules:           List of tuples [(rule:str, confidence:float), ... ]\n",
    "                             Example: [(\"(1, 2) => (5)\", 0.84), (\"(3, 4) => (7)\", 0.75)]\n",
    "    \"\"\"\n",
    "    \n",
    "    ### TODO Your code here\n",
    "    items = get_all_items(T)\n",
    "    k = 1\n",
    "    C = [[]]\n",
    "    L =  [[]]\n",
    "    C.append(listify(items))\n",
    "    \n",
    "    while len(C[k]) > 0:\n",
    "        print(\"k\", k)\n",
    "        \n",
    "        # frequent itemset generation\n",
    "        L.append([])\n",
    "        for I in C[k]:\n",
    "            if is_frequent(I, T, support):\n",
    "                L[k].append(I)\n",
    "        \n",
    "        # candidate generation\n",
    "        C.append([])\n",
    "        for l1 in L[k]:\n",
    "            for l2 in L[k]:\n",
    "                candidate = join_pair(l1, l2)\n",
    "                if len(candidate) == k+1:\n",
    "                    if is_frequent(candidate, T, support):\n",
    "                        if not contains(candidate, C[k+1]):\n",
    "                            C[k+1].append(candidate)\n",
    "        \n",
    "        k += 1\n",
    "    \n",
    "    # find rules\n",
    "    rules = []\n",
    "    itemsets = flatten(L)\n",
    "    for X in itemsets:\n",
    "        for Y in itemsets:\n",
    "            candidate = join_pair(X, Y)\n",
    "            if len(candidate) == len(X) + len(Y):\n",
    "                confidence = get_occurences(candidate, T) / get_occurences(X, T)\n",
    "                if confidence >= min_confidence:\n",
    "                    rules.append((rule(X, Y), confidence))\n",
    "    \n",
    "    \n",
    "    ### TODO Your code here\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'apriori_algorithm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5f0b38eca438>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapriori_algorithm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msupport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_confidence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mrules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrules\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%-8s \\t %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Conf.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Rule\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'apriori_algorithm' is not defined"
     ]
    }
   ],
   "source": [
    "rules = apriori_algorithm(T, support=0.05, min_confidence=0.7)\n",
    "rules = sorted(rules, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"%-8s \\t %s\" % (\"Conf.\", \"Rule\"))\n",
    "for r in rules:\n",
    "    print(\"%7.4f%% \\t %s\" % r[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequent_item_count_dict(T, support):\n",
    "    frequent_item_count = {}\n",
    "    all_item_count = {}\n",
    "    items = []\n",
    "    \n",
    "    # no we have the node/header table\n",
    "    for t in T:\n",
    "        for item in t:\n",
    "            if item in all_item_count.keys():\n",
    "                all_item_count[item]['count'] += 1\n",
    "                items += [item]\n",
    "            else:\n",
    "                all_item_count[item] = {'count': 0, 'node_pointer': None, 'word': item}\n",
    "    \n",
    "    for item in items:\n",
    "        if all_item_count[item]['count'] >= support: \n",
    "            frequent_item_count[item] = all_item_count[item]\n",
    "            \n",
    "    return frequent_item_count, items\n",
    "\n",
    "\n",
    "def build_fp_tree(T, support):\n",
    "    frequent_item_count, items = get_frequent_item_count_dict(T, support)\n",
    "    \n",
    "    fp_tree = {\n",
    "        'parent': None,\n",
    "        'children': {}, \n",
    "        'word_count': 1, \n",
    "        'link': None,\n",
    "        'word': None\n",
    "    }\n",
    "            \n",
    "    for transaction in T:\n",
    "        contains_frequent_item = False\n",
    "        for item in transaction:\n",
    "            if item in frequent_item_count.keys():\n",
    "                contains_frequent_item = True\n",
    "                \n",
    "        if not contains_frequent_item:\n",
    "            continue\n",
    "            \n",
    "        no_unfreq_transaction = []\n",
    "        for w in transaction: \n",
    "            if w in frequent_item_count.keys(): \n",
    "                no_unfreq_transaction +=[w]\n",
    "            \n",
    "        sorted_transaction = sorted(no_unfreq_transaction, key=lambda item: -frequent_item_count[item]['count'])\n",
    "        \n",
    "        current_node = fp_tree\n",
    "        for item in sorted_transaction:\n",
    "            current_children = current_node['children'].keys()\n",
    "            \n",
    "            if not item in current_children:\n",
    "                current_node['children'][item] = {\n",
    "                    'parent': current_node,\n",
    "                    'children': {}, \n",
    "                    'word_count': 0, \n",
    "                    'link': None,\n",
    "                    'word': item\n",
    "                }\n",
    "                if frequent_item_count[item]['node_pointer'] is None:\n",
    "                    frequent_item_count[item]['node_pointer'] = current_node\n",
    "                else:\n",
    "                    tmp_node = frequent_item_count[item]['node_pointer']\n",
    "                    while(tmp_node['link'] is not None): tmp_node = tmp_node['link']\n",
    "                    tmp_node['link'] = current_node\n",
    "                \n",
    "            current_node['children'][item]['word_count'] += 1\n",
    "            current_node = current_node['children'][item]\n",
    "            \n",
    "    return fp_tree, frequent_item_count\n",
    "\n",
    "\n",
    "def get_prefixes_of_item(item):\n",
    "    paths, counts = [], []\n",
    "    current_node = item['node_pointer']\n",
    "    \n",
    "    while current_node is not None:\n",
    "        path = []\n",
    "        traversal_node = current_node['parent']\n",
    "        while traversal_node is not None:\n",
    "            path += [traversal_node['word']]\n",
    "            traversal_node = traversal_node['parent']\n",
    "        \n",
    "        paths += [path]\n",
    "        counts += [current_node['word_count']]\n",
    "        current_node = current_node['link']\n",
    "       \n",
    "    return paths, counts\n",
    "\n",
    "\n",
    "def get_intersection(l1, l2):\n",
    "    intersection = []\n",
    "    \n",
    "    for item in l1:\n",
    "        if item in l2:\n",
    "            intersection += [item]\n",
    "    \n",
    "    return intersection\n",
    "\n",
    "def get_many_intersection(list_of_lists):\n",
    "    intersection = list_of_lists[0]\n",
    "            \n",
    "    for l in list_of_lists:\n",
    "        intersection = get_intersection(intersection, l)    \n",
    "    \n",
    "    return intersection\n",
    "\n",
    "\n",
    "def fp_growth(T, support):\n",
    "    \"\"\"\n",
    "        FPGrowth algorithm for mining frequent item sets.\n",
    "        Inputs:\n",
    "            T:                   A list of lists, each inner list will contiain integer-item-ids. \n",
    "                                 Example: T = [[1, 2, 5], [2, 3, 4], [1, 6]]\n",
    "            support:             The proportion of occurences needed to keep itemsets.\n",
    "        \n",
    "        Outputs:\n",
    "            frequent_itemsets:   List of frequent itemsets\n",
    "                                 Example: [[1, 2, 5], [1, 6]]\n",
    "    \"\"\"\n",
    "    fp_tree, frequent_item_count = build_fp_tree(T, support)\n",
    "    \n",
    "    frequent_patterns = []\n",
    "    \n",
    "    for item in frequent_item_count.keys():\n",
    "        paths, counts = get_prefixes_of_item(frequent_item_count[item])\n",
    "        \n",
    "        common_items = get_many_intersection(paths)\n",
    "        \n",
    "        count_sum = sum(counts)\n",
    "        subsets = []\n",
    "                \n",
    "        for i in range(len(common_items)):\n",
    "            subsets = [list(blu) for blu in itertools.combinations(common_items, i)]\n",
    "            \n",
    "        for subset in subsets:\n",
    "            frequent_patterns += [subset + [item]]\n",
    "            \n",
    "    return frequent_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-37940c25f7c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m              ['I1','I2','I3']]\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mfp_growth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-1f2748b51880>\u001b[0m in \u001b[0;36mfp_growth\u001b[1;34m(T, support)\u001b[0m\n\u001b[0;32m    120\u001b[0m                                  \u001b[0mExample\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \"\"\"\n\u001b[1;32m--> 122\u001b[1;33m     \u001b[0mfp_tree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrequent_item_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_fp_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msupport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[0mfrequent_patterns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-1f2748b51880>\u001b[0m in \u001b[0;36mbuild_fp_tree\u001b[1;34m(T, support)\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                     \u001b[0mtmp_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrequent_item_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'node_pointer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                     \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_node\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'link'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtmp_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_node\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'link'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m                     \u001b[0mtmp_node\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'link'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_node\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_data = [['I1','I2','I5'],\n",
    "             ['I2','I4'],\n",
    "             ['I2','I3'],\n",
    "             ['I1','I2','I4'],\n",
    "             ['I1','I3'],\n",
    "             ['I2','I3'],\n",
    "             ['I1','I3'],\n",
    "             ['I1','I2','I3','I5'],\n",
    "             ['I1','I2','I3']]\n",
    "\n",
    "fp_growth(test_data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - Random Walks\n",
    "We introduced the notion of hitting time, as the expected length of a random walk \n",
    "between two nodes; the expected number of steps before a simple random walk starting \n",
    "from a vertex $v$ reaches a vertex $u$. Your present task is to compute the average \n",
    "hitting time in the \n",
    "`cit-DBLP` dataset from the [citation dataset collection](http://networkrepository.com/cit.php) \n",
    "in the [Network Repository](http://networkrepository.com/networks.php).\n",
    "Here, average is defined across all pairs of nodes, considered in both directions. \n",
    "You may ignore pairs that are not connected by a (directed) path. \n",
    "Implement an algorithm that computes this average hitting time and report your \n",
    "result.\n",
    "\n",
    "_Hint:_ We have used this dataset before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges:  49743\n",
      "Number of nodes:  12591\n",
      "Position example:  [0.03738057 0.02658339]\n"
     ]
    }
   ],
   "source": [
    "# Method will load a list of all pairs of nodes that are \n",
    "# connected to each other by an edge.\n",
    "# Additionally, is will load precomputed positions for plotting nodes.\n",
    "# It is, however, not a pretty plot but the computatio\n",
    "edges, pos = load_dblp_citations()\n",
    "print(\"Number of edges: \", len(edges))\n",
    "print(\"Number of nodes: \", len(pos))\n",
    "print(\"Position example: \", pos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(G, u, v, max_walk_length=100):    \n",
    "    p = u\n",
    "    for k in range(max_walk_length):\n",
    "        neighbors = G.out_edges(p)\n",
    "        neighbors = listify(neighbors)\n",
    "        \n",
    "        if len(neighbors) == 0: \n",
    "            return -1\n",
    "        \n",
    "        p = random.choice(neighbors)[0][1]\n",
    "\n",
    "        if p == v:\n",
    "            return k\n",
    "        \n",
    "    return -1\n",
    "\n",
    "def hitting_time(G, n1, n2, k=10, max_walk_length=100):\n",
    "    walk_lengths = []\n",
    "    \n",
    "    for _ in range(k):\n",
    "        walk_length = random_walk(G, n1, n2, max_walk_length)\n",
    "        \n",
    "        if walk_length > 0:\n",
    "            walk_lengths.append(walk_length)\n",
    "    \n",
    "    if len(walk_lengths) == 0:\n",
    "        return 0\n",
    "    \n",
    "    return np.mean(walk_lengths)\n",
    "\n",
    "def avg_hitting_time(G):\n",
    "    nodes = nx.nodes(G)\n",
    "\n",
    "    total_hitting_time = 0\n",
    "\n",
    "    for n1 in nodes:\n",
    "        for n2 in nodes:\n",
    "            ht = hitting_time(G, n1, n2)\n",
    "            total_hitting_time += ht\n",
    "\n",
    "\n",
    "    average_hitting_time = total_hitting_time / (len(nodes) * len(nodes))\n",
    "    \n",
    "    return average_hitting_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.read_edgelist(\"../exercises/cit-DBLP.edges\", nodetype=int).to_directed()\n",
    "\n",
    "#G = nx.DiGraph()\n",
    "#G.add_edges_from([(1, 4), (2, 3), (2, 5), (3, 1), \n",
    "#                  (4, 3), (5, 4), (5, 2), (5, 6), \n",
    "#                  (5, 3), (6, 3), (6, 0), (7, 1),  \n",
    "#                  (7, 3), (0, 1)])\n",
    "\n",
    "avg_hitting_time(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 - Sequence Segmentation\n",
    "The Dynamic Programming algorithm for optimally segmenting a sequence $S$ of length $n$ \n",
    "into $B$ segments, that we have introduced, is expressed by the following recursive equation:\n",
    "\n",
    "$$\n",
    "E(i, b) = \\min_{j < i}\\left[ E(j, b-1) + Err(j+1, i)\\right]\n",
    "$$\n",
    "\n",
    "where $Err(j+1, i)$ is the error of a segment that contains items from $j+1$ to $i$.\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "**1. What is the default space-complexity of this algorithm?**\n",
    "N * B\n",
    "\n",
    "**2. If we are willing to recompute some tabulated results, can we then reduce the \n",
    "    default space-complexity? _Exactly how_? What is the space-complexity then?**\n",
    "\n",
    "\n",
    "    \n",
    "**3. What is the cost of using the above space-efficiency technique in terms of time-complexity?**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**4. For the sub-problem of segmenting the $i$-prefix of sequence $S$ into $b$ segments, consider \n",
    "    the segment $M(i, b)$ that contains (if such segment exists) the middle item of \n",
    "    index $\\lfloor \\frac{n}{2} \\rfloor$. The boundaries of $M(i, b)$ can be detected and tabulated \n",
    "    along with each $E(i, b)$ solution. Based on this observation, devise a method that reduces \n",
    "    the time-complexity burden identified in (3). \n",
    "    _(hint: use [divide-and-conquer](https://en.wikipedia.org/wiki/Divide-and-conquer_algorithm))_**\n",
    "    \n",
    "    \n",
    "**5. What is the time complexity when using the technique proposed in (4)?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:** \n",
    "As this is the final handin and we are getting close to the exam, rehandins are not an option.\n",
    "Therefor, we strongly encourage you to get started early and if you get too stuck, make sure\n",
    "to send Frederik an email early (not right up to the deadline). For a faster reply, use\n",
    "[fhvilshoj@cs.au.dk](mailto:fhvilshoj@cs.au.dk) and **not** ~201206000@ post.au.dk~. \n",
    "\n",
    "For those of you, who are not used to analyzing algorithms: by time-complexity and space-complexity, \n",
    "we refer to the theoretical computation time and memory usage, respectively, as a function of the problem size, i.e., as a \n",
    "function of $n$ and $B$ in Problem 3. We use [Big O notation](https://en.wikipedia.org/wiki/Big_O_notation)\n",
    "to specify this. You should **not** infer it by implementing it in practice ;-) \n",
    "Again, when in doubt, shoot Frederik an email. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
